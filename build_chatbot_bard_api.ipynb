{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO7oyQ0npbxEGLefK1h6NxA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdullahmiraz/chatbot-google-generative-ai/blob/main/build_chatbot_bard_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai"
      ],
      "metadata": {
        "id": "885AZxB6ttXQ",
        "outputId": "50dfc55c-ad35-4896-ab55-e6b8724d12f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.3.3 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.3.3)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.17.3)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.11.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.3.3->google-generativeai) (1.22.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.61.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.31.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai) (4.9)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.59.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth->google-generativeai) (0.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as palm\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "nP6nICY9tvK4"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "palm.configure(api_key='AIzaSyAcEViPJjklrV0uc4rpM-hwFUw9Oo7ydFE')"
      ],
      "metadata": {
        "id": "uZh4bljnuFML"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the palm.list_models function to find available models:\n",
        "models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
        "model = models[0].name\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1vP4lgIuNpi",
        "outputId": "c4352e19-680e-4663-e45f-f6e6e4ce97ac"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/text-bison-001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Simple Chatbot**"
      ],
      "metadata": {
        "id": "6gdB4NULuS-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = input(\"Enter your question and what do you expect? : \")\n",
        "\n",
        "completion = palm.generate_text(\n",
        "    model=model,\n",
        "    prompt=prompt,\n",
        "    temperature=0,\n",
        "    # The maximum length of the response\n",
        "    max_output_tokens=800,\n",
        ")\n",
        "\n",
        "print(completion.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGlzC9gyubMB",
        "outputId": "325cd05d-8608-4151-e59a-955521b26ebd"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question and what do you expect? : how old are you ?\n",
            "22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Text Summrizer**"
      ],
      "metadata": {
        "id": "RXkMCzZDvZpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj3OJg3mvhpK",
        "outputId": "c9b6a5a7-e434-4872-a31d-1be1cb18a9f2"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (3.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader"
      ],
      "metadata": {
        "id": "vW50UqhavliU"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChFEk_TgvqiY",
        "outputId": "475f27d1-8387-4fda-d84c-b0c19341b35f"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directory = '/content/drive/MyDrive/Colab-Notebooks/Build-A-Chatbot-Project-Bard-API/'\n",
        "filename  = 'Attention-Is-All-You-Need.pdf'\n",
        "# filename  = 'Art-of-War.pdf'"
      ],
      "metadata": {
        "id": "_ZKXg0PjvsTk"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a pdf file object\n",
        "pdfFileObject = open(directory+filename, 'rb')\n",
        "# creating a pdf reader object\n",
        "pdfReader = PdfReader(pdfFileObject)\n",
        "text=[]\n",
        "summary=' '\n",
        "#Storing the pages in a list\n",
        "for i in range(0,len(pdfReader.pages)):\n",
        "  # creating a page object\n",
        "  pageObj = pdfReader.pages[i].extract_text()\n",
        "  pageObj= pageObj.replace('\\t\\r','')\n",
        "  pageObj= pageObj.replace('\\xa0','')\n",
        "  # extracting text from page\n",
        "  text.append(pageObj)"
      ],
      "metadata": {
        "id": "SbVlYvzuv45x"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge multiple page - to reduce API Calls\n",
        "def join_elements(lst, chars_per_element):\n",
        "    new_lst = []\n",
        "    for i in range(0, len(lst), chars_per_element):\n",
        "        new_lst.append(''.join(lst[i:i+chars_per_element]))\n",
        "    return new_lst\n",
        "\n",
        "# Option to keep x elements per list element\n",
        "new_text = join_elements(text, 3)\n",
        "\n",
        "print(f\"Original Pages = \",len(text))\n",
        "print(f\"Compressed Pages = \",len(new_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c7APyHpyupm",
        "outputId": "5764b8cc-e594-4118-95ff-68a791362a92"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Pages =  15\n",
            "Compressed Pages =  5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt):\n",
        "  completion = palm.generate_text(model=model,\n",
        "                                  prompt=prompt,\n",
        "                                  temperature=0,\n",
        "                                  # The maximum length of the response\n",
        "                                  max_output_tokens=200,\n",
        "                                  )\n",
        "  return completion.result"
      ],
      "metadata": {
        "id": "95PzqdIWyy-F"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = \"\"\n",
        "for i in range(len(new_text)):\n",
        "  prompt =f\"\"\"\n",
        "  Your task is to act as a Text Summariser.\n",
        "  I'll give you text from  pages of a book from beginning to end.\n",
        "  And your job is to summarise text from these pages in less than 100 words.\n",
        "  Don't be conversational. I need a plain 100 word answer.\n",
        "  Text is shared below, delimited with triple backticks:\n",
        "  ```{text[i]}```\n",
        "  Write not more than 15 sentences\n",
        "  \"\"\"\n",
        "  try:\n",
        "    response = get_completion(prompt)\n",
        "  except:\n",
        "    response = get_completion(prompt)\n",
        "  print(response)\n",
        "  summary= f\"{summary} {response}\\n\\n\"\n",
        "  # result.append(response)\n",
        "  time.sleep(19)  #You can query the model only 3 times in a minute for free, so we need to put some delay"
      ],
      "metadata": {
        "id": "bNckgADvy1vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(directory +'/palm_api_summary.txt',\n",
        "          'w') as out:\n",
        "  out.write(summary)"
      ],
      "metadata": {
        "id": "An6hrV5wzDli"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Mock Interviewer chatbot**"
      ],
      "metadata": {
        "id": "JjofVaD4zGaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "domain = input(\"Enter the field which you are doing interview ?: (ex. Software engineering: )\")\n",
        "quantity  = input(\"How many questions ?\")\n",
        "prompt = f\"\"\"\n",
        "  you are a very expert in taking interview and asking question to check candidates.\n",
        "  you are going to take interview and ask question regarding the {domain} and {quantity} questions. Ask questions only\n",
        "\"\"\"\n",
        "\n",
        "completion = palm.generate_text(\n",
        "    model=model,\n",
        "    prompt=prompt,\n",
        "    temperature=0,\n",
        "    # The maximum length of the response\n",
        "    max_output_tokens=800,\n",
        ")\n",
        "\n",
        "# Get the answer from the user\n",
        "answer = input(completion.result + \"\\n--------------------------------------\\n\" + \"Answer: \")\n",
        "\n",
        "# Prompt Palm to check the answer and provide feedback\n",
        "prompt = f\"\"\" Check the answers {answer}\n",
        "   if its not correct explain a little bit or give hints\n",
        "\"\"\"\n",
        "\n",
        "# Generate feedback\n",
        "completion = palm.generate_text(\n",
        "    model=model,\n",
        "    prompt=prompt,\n",
        "    temperature=0,\n",
        "    # The maximum length of the response\n",
        "    max_output_tokens=800,\n",
        ")\n",
        "print(\"--------------------------------------------\\nAnswer by AI\")\n",
        "print(completion.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BDf889bzKho",
        "outputId": "434f3644-a967-45f2-a782-e75019a81cc4"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the field which you are doing interview ?: (ex. Software engineering: )computer science\n",
            "How many questions ?2\n",
            "1. What is the difference between a compiler and an interpreter?\n",
            "2. What is the difference between a static and a dynamic language?\n",
            "--------------------------------------\n",
            "Answer: **1. Compiler**  * **Converts** the entire program into machine code before it is run. * **Faster** at runtime. * **Less flexible** than interpreters.  **2. Interpreter**  * **Translates** the program line by line as it is run. * **Slower** at runtime. * **More flexible** than compilers.  **Static language**  * **Checks** data types and other errors at compile time. * **Faster** at runtime.  **Dynamic language**  * **Checks** data types and other errors at runtime. * **Slower** at runtime.  **In 5 sentences:**  **Compiler:** Translates the entire program into machine code before it is run, resulting in faster runtime but less flexibility.  **Interpreter:** Translates the program line by line as it is run, resulting in slower runtime but more flexibility.  **Static language:** Checks data types and other errors at compile time, resulting in faster runtime.  **Dynamic language:** Checks data types and other errors at runtime, resulting in slower runtime.\n",
            "--------------------------------------------\n",
            "Answer by AI\n",
            "The answer is correct.\n",
            "\n",
            "Compiler converts the entire program into machine code before it is run, resulting in faster runtime but less flexibility.\n",
            "Interpreter translates the program line by line as it is run, resulting in slower runtime but more flexibility.\n",
            "Static language checks data types and other errors at compile time, resulting in faster runtime.\n",
            "Dynamic language checks data types and other errors at runtime, resulting in slower runtime.\n"
          ]
        }
      ]
    }
  ]
}