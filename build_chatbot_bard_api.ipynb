{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO7oyQ0npbxEGLefK1h6NxA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdullahmiraz/chatbot-google-generative-ai/blob/main/build_chatbot_bard_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai"
      ],
      "metadata": {
        "id": "885AZxB6ttXQ",
        "outputId": "50dfc55c-ad35-4896-ab55-e6b8724d12f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.3.3 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.3.3)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.17.3)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.11.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.3.3->google-generativeai) (1.22.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.61.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.31.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai) (4.9)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.59.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth->google-generativeai) (0.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as palm\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "nP6nICY9tvK4"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "palm.configure(api_key='AIzaSyAcEViPJjklrV0uc4rpM-hwFUw9Oo7ydFE')"
      ],
      "metadata": {
        "id": "uZh4bljnuFML"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the palm.list_models function to find available models:\n",
        "models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
        "model = models[0].name\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1vP4lgIuNpi",
        "outputId": "c4352e19-680e-4663-e45f-f6e6e4ce97ac"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/text-bison-001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Simple Chatbot**"
      ],
      "metadata": {
        "id": "6gdB4NULuS-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = input(\"Enter your question and what do you expect? : \")\n",
        "\n",
        "completion = palm.generate_text(\n",
        "    model=model,\n",
        "    prompt=prompt,\n",
        "    temperature=0,\n",
        "    # The maximum length of the response\n",
        "    max_output_tokens=800,\n",
        ")\n",
        "\n",
        "print(completion.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGlzC9gyubMB",
        "outputId": "325cd05d-8608-4151-e59a-955521b26ebd"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question and what do you expect? : how old are you ?\n",
            "22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Text Summrizer**"
      ],
      "metadata": {
        "id": "RXkMCzZDvZpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj3OJg3mvhpK",
        "outputId": "c9b6a5a7-e434-4872-a31d-1be1cb18a9f2"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (3.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader"
      ],
      "metadata": {
        "id": "vW50UqhavliU"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChFEk_TgvqiY",
        "outputId": "475f27d1-8387-4fda-d84c-b0c19341b35f"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directory = '/content/drive/MyDrive/Colab-Notebooks/Build-A-Chatbot-Project-Bard-API/'\n",
        "filename  = 'Attention-Is-All-You-Need.pdf'\n",
        "# filename  = 'Art-of-War.pdf'"
      ],
      "metadata": {
        "id": "_ZKXg0PjvsTk"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a pdf file object\n",
        "pdfFileObject = open(directory+filename, 'rb')\n",
        "# creating a pdf reader object\n",
        "pdfReader = PdfReader(pdfFileObject)\n",
        "text=[]\n",
        "summary=' '\n",
        "#Storing the pages in a list\n",
        "for i in range(0,len(pdfReader.pages)):\n",
        "  # creating a page object\n",
        "  pageObj = pdfReader.pages[i].extract_text()\n",
        "  pageObj= pageObj.replace('\\t\\r','')\n",
        "  pageObj= pageObj.replace('\\xa0','')\n",
        "  # extracting text from page\n",
        "  text.append(pageObj)"
      ],
      "metadata": {
        "id": "SbVlYvzuv45x"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge multiple page - to reduce API Calls\n",
        "def join_elements(lst, chars_per_element):\n",
        "    new_lst = []\n",
        "    for i in range(0, len(lst), chars_per_element):\n",
        "        new_lst.append(''.join(lst[i:i+chars_per_element]))\n",
        "    return new_lst\n",
        "\n",
        "# Option to keep x elements per list element\n",
        "new_text = join_elements(text, 3)\n",
        "\n",
        "print(f\"Original Pages = \",len(text))\n",
        "print(f\"Compressed Pages = \",len(new_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c7APyHpyupm",
        "outputId": "5764b8cc-e594-4118-95ff-68a791362a92"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Pages =  15\n",
            "Compressed Pages =  5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt):\n",
        "  completion = palm.generate_text(model=model,\n",
        "                                  prompt=prompt,\n",
        "                                  temperature=0,\n",
        "                                  # The maximum length of the response\n",
        "                                  max_output_tokens=200,\n",
        "                                  )\n",
        "  return completion.result"
      ],
      "metadata": {
        "id": "95PzqdIWyy-F"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = \"\"\n",
        "for i in range(len(new_text)):\n",
        "  prompt =f\"\"\"\n",
        "  Your task is to act as a Text Summariser.\n",
        "  I'll give you text from  pages of a book from beginning to end.\n",
        "  And your job is to summarise text from these pages in less than 100 words.\n",
        "  Don't be conversational. I need a plain 100 word answer.\n",
        "  Text is shared below, delimited with triple backticks:\n",
        "  ```{text[i]}```\n",
        "  Write not more than 15 sentences\n",
        "  \"\"\"\n",
        "  try:\n",
        "    response = get_completion(prompt)\n",
        "  except:\n",
        "    response = get_completion(prompt)\n",
        "  print(response)\n",
        "  summary= f\"{summary} {response}\\n\\n\"\n",
        "  # result.append(response)\n",
        "  time.sleep(19)  #You can query the model only 3 times in a minute for free, so we need to put some delay"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "bNckgADvy1vn",
        "outputId": "5593f098-78c3-4713-a42f-82b84e75accb"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer is a new neural network architecture for sequence transduction tasks. It is based on attention mechanisms and does not use recurrence or convolutions. Experiments on two machine translation tasks show that Transformer models are superior in quality while being more parallelizable and requiring significantly less time to train.\n",
            "Transformer is a new neural network architecture for sequence transduction tasks. It eschews recurrence and instead relies entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-b359cdf5d5e4>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0msummary\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34mf\"{summary} {response}\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m# result.append(response)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#You can query the model only 3 times in a minute for free, so we need to put some delay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(directory +'/palm_api_summary.txt',\n",
        "          'w') as out:\n",
        "  out.write(summary)"
      ],
      "metadata": {
        "id": "An6hrV5wzDli"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Mock Interviewer chatbot**"
      ],
      "metadata": {
        "id": "JjofVaD4zGaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "domain = input(\"Enter the field which you are doing interview ?: (ex. Software engineering: )\")\n",
        "quantity  = input(\"How many questions ?\")\n",
        "prompt = f\"\"\"\n",
        "  you are a very expert in taking interview and asking question to check candidates.\n",
        "  you are going to take interview and ask question regarding the {domain} and {quantity} questions. Ask questions only\n",
        "\"\"\"\n",
        "\n",
        "completion = palm.generate_text(\n",
        "    model=model,\n",
        "    prompt=prompt,\n",
        "    temperature=0,\n",
        "    # The maximum length of the response\n",
        "    max_output_tokens=800,\n",
        ")\n",
        "\n",
        "# Get the answer from the user\n",
        "answer = input(completion.result + \"\\n--------------------------------------\\n\" + \"Answer: \")\n",
        "\n",
        "# Prompt Palm to check the answer and provide feedback\n",
        "prompt = f\"\"\" Check the answers {answer}\n",
        "   if its not correct explain a little bit or give hints\n",
        "\"\"\"\n",
        "\n",
        "# Generate feedback\n",
        "completion = palm.generate_text(\n",
        "    model=model,\n",
        "    prompt=prompt,\n",
        "    temperature=0,\n",
        "    # The maximum length of the response\n",
        "    max_output_tokens=800,\n",
        ")\n",
        "print(\"--------------------------------------------\\nAnswer by AI\")\n",
        "print(completion.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BDf889bzKho",
        "outputId": "3f687679-4bfe-4de5-d56b-ff7d0dc8b466"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the field which you are doing interview ?: (ex. Software engineering: )cs\n",
            "How many questions ?5\n",
            "1. Why do you want to work in the field of computer science?\n",
            "2. What are your strengths and weaknesses as a computer scientist?\n",
            "3. What are your goals for the future?\n",
            "4. What are your salary expectations?\n",
            "5. What are your thoughts on the current state of the tech industry?--------------------------------------\n",
            "Answer: dont know\n",
            "--------------------------------------------\n",
            "Answer by AI\n",
            "1. The answer is \"the United States\".\n",
            "\n",
            "The United States is the only country in the world that has a flag with 50 stars.\n"
          ]
        }
      ]
    }
  ]
}